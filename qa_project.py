# -*- coding: utf-8 -*-
"""QA_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gc-ehZzJrE4dLW2Ka4O0binAaZ717NNN
"""

!pip3 install python-dotenv

from dotenv import load_dotenv
load_dotenv()

!pip install -qU langchain-teddynote
from langchain_teddynote import logging

logging.langsmith("QA_project.ipynb")

!pip install langchain-community langchain-core

!pip install langchain-openai

import bs4
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

loader = WebBaseLoader(
    web_paths=("https://n.news.naver.com/mnews/article/001/0015197166?rc=N&ntype=RANKING",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            "div",
            attrs={"class": ["newsct_article _article_body", "media_end_head_title"]},
        )
    ),
)

docs = loader.load()
print(f"문서의 수: {len(docs)}")
docs

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

splits = text_splitter.split_documents(docs)
len(splits)

!pip install chromadb

vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.
검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.
한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.

#Question:
{question}

#Context:
{context}

#Answer:"""
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

answer = rag_chain.stream("최근 제주도적설량에 대해서 설명해주세요.")
stream_response(answer)

answer = rag_chain.stream("최근 경기남부 적설량에 대해서 설명해주세요.")
stream_response(answer)